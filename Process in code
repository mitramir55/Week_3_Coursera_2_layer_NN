# Week_3_Coursera_2_layer_NN
This is the neat representation of what I've learned from Andrew Ng course third week homework assignment.

import numpy as np
import matplotlib.pyplot as plt
import h5py
import scipy
from PIL import Image
from scipy import ndimage
#from lr_utils import load_dataset

%matplotlib inline

# 1. Asserting the shape of each

def layer_s(X, Y):
    n_x = X.shape[0]
    n_y = Y.shape[0]
    n_h = 4
    return (n_x, n_h, n_y)
    
    
# 2. Initializing weights and biases

def initialize_params(n_x, n_y, n_h):
    w1 = np.random.randn(n_h, n_x) * 0.01
    b1 = np.zeros((n_h, 1))
    w2 = np.random.randn(n_y, h_h)
    b2 = 0
    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}
    return parameters
    
    
def forward_prop(X,parameters):
    W1 = parameters['W1']
    W2 = parameters['W2']
    b1 = parameters['b1']
    b2 = parameters['b2']
    
    Z1 = np.dot(W1, X) + b1
    A1 = np.tanh(Z1)
    
    Z2 = np.dot(W2, a1) + b2
    A2 = sigmoid(Z2)
    
    assert(A2.shape == (1, X.shape[1]))
    
    cache = {
        'Z1': Z1,
        'Z2':Z2,
        'A1': A1,
        'A2': A2
    }
    return A2, cache
    
 def compute_cost(A2, Y):
    cost =\
    -(1/m) * (np.dot((Y), np.log(A2)) + \
                   np.dot((1-Y), np.log(1-A2)))
    cost = float(np.squeeze(cost))
    return cost
    
    
    
# 5. Backprop



def backward_prop(params, cache, X, Y):
    A2, cache = forward_prop(X,parameters)
    W1 = params['W1']
    W2 = params['W2']

    
    A1 = cache['A1']
    Z1 = cache['Z1']
    Z2 = cache['Z2']
    
    
    dZ2 = A2 - Y
    dW2 = (1/m) * np.dot(dZ2, A1.T)
    db2 = (1/m) * np.sum(dZ2)
    
    dZ1 = np.dop(W2.T, dZ2)*(1- np.power(A1, 2))
    dW1 = (1/m) * np.dot(dZ1, X.T)
    db1 = (1/m) * np.sum(dZ1, axis = 1, keepdims = Ture)
    
    grads = {
        'dW1': dW1,
        'dW2': dW2,
        'db1': db1,
        'db2': db2,
    }
    return grads
    
    
 def update_params(parameters, grad, learning_rate = 1.2):
    # from param dictionary
    W1 = parameters['W1']
    W2 = parameters['W2']
    b1 = parameters['b1']
    b2 = parameters['b2']
    
    # from grads dictionary
    dW1 = grads['dW1']
    dW2 = grads['dW2']
    db1 = grads['db1']
    db2 = grads['db2']
    
    # calculating new params
    W1 = W1 - lr * dW1
    W2 = W2 - lr * dW2
    b1 = b1 - lr * db1
    b2 = b2 - lr * db2
    
    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}
    return parameters
    
def NN_model(X, Y,n_h num_iter = 100):
    
    cost_list = []
    # defining the structure and params
    n_x, n_y = layer_s(X, Y)[0], layer_s(X, Y)[2]
    parameters = initialize_params(n_x, n_h, n_y)
    
    for i in range(num_iter):
        A2, cache = forward_prop(X,parameters)
        cost = compute_cost(A2, Y)
        cost_list.append(cost)
        grads = backward_prop(params, cache, X, Y)
        parameters = update_params(grads, parameters, lr = 1.2)
    print('After {} iterations, learning rate of {.2f} our cost will be'.format(lr, cost))
    return parameters
    
    
 def predict():
    A2, cache = forward_prop(X,parameters)
    pred = A2>0.5
    return prediction
    
 
 
